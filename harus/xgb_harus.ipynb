{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Imports/Installs/Global Vars</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Installing required packages (if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsPath = '../datasets/harus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Dataset.csv<br>\n",
    "*Since the data is already split into label, values, training and test, this subdivision is being preserved while importing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing list of feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the features names for the dataset\n",
    "# Features und deren Werte sind in verschiedenen txt-Dateien gespeichert\n",
    "# Es ist daher notwendig die txt-Dateien separat auszulesen, aufzuarbeiten und zu einem Dataframe zusammenzufuegen\n",
    "# Die Daten folgen dabei keiner gaengingen Formatierung\n",
    "# features.txt teilt die einzelnen Eintraege nach Spalten\n",
    "# Jede Spalte besteht aus einem Index und dem Namen des Features, getrennt durch ein Leerzeichen\n",
    "\n",
    "# Da die Spaltennamen teilweise mehrfach auftauchen, werden die Namen der Duplikate um die Anzahl der\n",
    "# bisherigen Vorkommen erweitert\n",
    "# Beispiel: Die Spalte \"BodyGyroZ\" wird als \"BodyGyroZ_1\" und \"BodyGyroZ_2\" erweitert\n",
    "\n",
    "with open(datasetsPath + 'UCI HAR Dataset/features.txt') as f:\n",
    "    xNames = [] # List of column/feature names\n",
    "\n",
    "    for line in f:                          # Reading each line\n",
    "        parts = line.strip().split(' ')     # Splitting each line by space\n",
    "\n",
    "        if len(parts) > 1:                  # If the line has more than 1 element\n",
    "            label = parts[1]                # The second element is the label\n",
    "            dupe = False                    # Label is not a dupe, yet!\n",
    "            index = 0                       # Rising index for naming\n",
    "            amount = xNames.count(label)    # Check, if the label is already in the list\n",
    "            \n",
    "            while amount!=0:                # If a label is already in the list, add a number (index) to it \n",
    "                index = index+1\n",
    "                newName = label + '_' + str(index)\n",
    "                amount = xNames.count(newName)\n",
    "                dupe = True\n",
    "            if dupe == True:                # If the label has been a duplicate, use the new name\n",
    "                xNames.append(newName)    \n",
    "            else:\n",
    "                xNames.append(label)        # Otherwise, just use the original name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing feature values and creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Trainingsdaten sind ebenfalls in einer txt-Datei gespeichert\n",
    "# Jede Reihe in einem Dataframe wird durch eine Zeile in der txt-Datei repraesentiert\n",
    "# Die einzelnen Werte werden durch (eine variierende Anzahl an ) Leerzeichen getrennt\n",
    "# Beim Einlesen werden die Zahlenwerte zudem als Strings erkannt und muessen noch gecastet werden\n",
    "\n",
    "xtrain = pd.DataFrame(columns=xNames)         # Create dataframes with columns named after features.txt\n",
    "xtest = pd.DataFrame(columns=xNames)\n",
    "\n",
    "with open(datasetsPath + 'UCI HAR Dataset/train/X_train.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        list = line.strip().split(' ')      # Create a list of every object in the list thats seperated by \" \"\n",
    "        list = [i for i in list if i != ''] # Remove empty strings\n",
    "        list = [float(i) for i in list]     # Cast every object in the list to float\n",
    "        xtrain.loc[len(xtrain)] = list          # Add new_list as a new row to the dataframe\n",
    "\n",
    "with open(datasetsPath + 'UCI HAR Dataset/test/X_test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        list = line.strip().split(' ')      # Create a list of every object in the list thats seperated by \" \"\n",
    "        list = [i for i in list if i != ''] # Remove empty strings\n",
    "        list = [float(i) for i in list]     # Cast every object in the list to float\n",
    "        xtest.loc[len(xtest)] = list          # Add new_list as a new row to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing labels (Labels are being transformed from 1:6 to 0:5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = pd.DataFrame(columns=['label'])    # Create dataframes for feature labels\n",
    "ytest = pd.DataFrame(columns=['label'])\n",
    "\n",
    "with open(datasetsPath + 'UCI HAR Dataset/train/y_train.txt', 'r') as f:\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        labels.append(int(line.strip())-1)\n",
    "    \n",
    "ytrain['label'] = labels\n",
    "\n",
    "with open(datasetsPath + 'UCI HAR Dataset/test/y_test.txt', 'r') as f:\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        labels.append(int(line.strip())-1)\n",
    "    \n",
    "ytest['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Data Preprocessing and Training</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in xtrain.columns:\n",
    "    xtrain[col] = xtrain[col].astype('float32')\n",
    "\n",
    "for col in ytrain.columns:\n",
    "    ytrain[col] = ytrain[col].astype('int32')\n",
    "\n",
    "evaldata=[(xtrain,ytrain),(xtest,ytest)]          # Datensatz zur Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Improving<br>\n",
    "Um eine gute Anzahl an Estimators zu bestimmen, wird zuerst ein Modell mithilfe von Early Stopping, sowie einer großen Menge an Estimatoren trainiert. Hiermit wird die beste Anzahl an Iterationen ermittelt und mit dieser Anzahl ein weiteres Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor = XGBClassifier(              # \"Spendermodell\"\n",
    "    objective='multi:softmax',      # Multi-Klassifizierung\n",
    "    num_class=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=10000,             # \"Große Anzahl an Schaetzern, die nicht erreicht werden soll\"\n",
    "    # early_stopping_rounds=50,       # Anzahl an Runden, bei denen sich das Modell nicht verbessern muss, bis abgebrochen wird\n",
    "    # max_depth=3                   # Erstmal weglassen\n",
    ")\n",
    "\n",
    "# Donor model\n",
    "# donor.fit(xtrain, ytrain, eval_set=evaldata, verbose=False)\n",
    "donor.fit(xtrain, ytrain)\n",
    "\n",
    "\n",
    "bIter = donor.best_iteration        # Beste Anzahl an Estimatoren\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softmax',  # Specify the multi-class classification task\n",
    "    num_class=6,                # Number of classes (Low, Moderate, High)\n",
    "    learning_rate=0.1,          # Learning rate for the model\n",
    "    n_estimators=bIter,         # Number of boosting rounds (iterations)\n",
    "    num_parallel_tree=1         # m2c workaround\n",
    "    # max_depth=3,              # Maximum depth of the trees\n",
    ")\n",
    "\n",
    "model.fit( # Final model\n",
    "    xtrain, \n",
    "    ytrain, \n",
    "    eval_set=evaldata, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(model.classes_)\n",
    "\n",
    "yhat = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Func Definitions</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(): # Confusion Matrix\n",
    "    metrics.ConfusionMatrixDisplay.from_estimator(model, xtest, ytest, cmap='Blues')\n",
    "    pyplot.show()\n",
    "\n",
    "def plotLossCurves():       # Loss Curves\n",
    "    # save evaluation results\n",
    "    results = model.evals_result()\n",
    "    # plot curves\n",
    "    lossValue = list(results['validation_1'])[0]\n",
    "    pyplot.plot(results['validation_0'][lossValue], label='train')\n",
    "    pyplot.plot(results['validation_1'][lossValue], label='train')\n",
    "    # show the legend\n",
    "    pyplot.xlabel('Iterations')\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "def printClassReport():     # Classification Report\n",
    "    # Report\n",
    "    print(metrics.classification_report(ytest, yhat, digits = 3))\n",
    "\n",
    "def printMisc():            # Best Iter, Test Accuracy, Base Score, Probas,\n",
    "    # Misc\n",
    "    print(f'# Trees / Best Iteration: \\t{bIter}')\n",
    "    print(f'Test Accuracy: \\t{accuracy_score(ytest, yhat)}')\n",
    "    print(f'Base_Score{model.base_score}')\n",
    "    print(f'\\nPredict_Proba Return: \\n{model.predict_proba(xtest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Porting this Bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portToC(model):\n",
    "    import m2cgen as m2c\n",
    "\n",
    "    with open('../exported_models/currentExport.c','w') as f:\n",
    "        code = m2c.export_to_c(model)\n",
    "        f.write(code)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Code for Lazy People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInfer(start=0, size=500, csv=True, float=True):\n",
    "    start = start\n",
    "    size = size\n",
    "    length = 2\n",
    "    \n",
    "    # Declaring function\n",
    "    print(f'void infer(int time, int csv) {{')\n",
    "\n",
    "    # Printing Header\n",
    "    print(f'\\tif(csv==1){{')\n",
    "    print(f'\\t\\tSerial.println(\"aScore0,aScore1\");        // Printing header to name columns in csv')\n",
    "    print(f'\\t}} else {{')\n",
    "    print(f'\\t\\tSerial.println(\"Start: {start} | End: {start+size}\");    // Printing Range:')\n",
    "    print(f'\\t}}')\n",
    "\n",
    "    print(f'\\t// Declarations:')\n",
    "    print(f'\\tint length = {length};')\n",
    "    if float == True:\n",
    "        print(f'\\tfloat result[length];')\n",
    "    else:\n",
    "        print(f'\\tdouble result[length];')\n",
    "\n",
    "    print(f'\\t// Model Inference')\n",
    "    for x in range(start,(start+size)):  \n",
    "        \n",
    "        if float == True:\n",
    "            print(f'\\tfloat x_{x}[] = {{' , end=\"\")    \n",
    "        else:\n",
    "            print(f'\\tdouble x_{x}[] = {{' , end=\"\")\n",
    "\n",
    "        features = xtest.values[x]\n",
    "\n",
    "        for i in range(len(features)):\n",
    "            if i < (len(features)-1):\n",
    "                print(features[i], end=\", \")\n",
    "            else:\n",
    "                print(features[i], end=\"};\\n\")\n",
    "\n",
    "        if csv == False:\n",
    "            print(f'\\tint y_{x} = {yhat[x]};')\n",
    "\n",
    "        print(f'\\tscore(x_{x}, result);')\n",
    "        \n",
    "        if csv == True:\n",
    "            print(f'\\tprintScoreCSV(result, length);')\n",
    "        else:\n",
    "            print(f'\\tprintScoreCompare(result, length, y_{x});')\n",
    "        \n",
    "        print(f'\\tdelay(time);\\n')\n",
    "    print(f'}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Inference Data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProbDF(localCapture=model,features_test=xtest):\n",
    "    xtestlist = localCapture.predict_proba(features_test).tolist()\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "\n",
    "    for x in xtestlist:\n",
    "        list1.append(round(x[0],4))\n",
    "        list2.append(round(x[1],4))\n",
    "        list3.append(round(x[2],4))\n",
    "\n",
    "    probDF = pd.DataFrame({\n",
    "        'Label': localCapture.predict(features_test),\n",
    "        'Prob0': list1,\n",
    "        'Prob1': list2,\n",
    "        'Prob2': list3\n",
    "    })\n",
    "    return(probDF)\n",
    "\n",
    "def exportProbDF(probDF = generateProbDF()):\n",
    "    probDF.to_csv(datasetsPath + 'baseCapture.csv')\n",
    "\n",
    "def importInoCapture():\n",
    "    serial = pd.read_csv(datasetsPath + 'inoCapture.csv')\n",
    "    serial = serial.truncate(after=(len(serial)-2)) # get rid of ##### REPEATING... #####\n",
    "    return(serial)\n",
    "\n",
    "def generateComparison(probDF=generateProbDF(),inoCapture=importInoCapture()):\n",
    "    probDF = probDF.truncate(after=(len(inoCapture)-1))\n",
    "    probDF = probDF.join(inoCapture)\n",
    "    probDF = probDF[['Label', 'Prob0', 'aScore0', 'Prob1', 'aScore1', 'Prob2', 'aScore2']] # Rearranging columns\n",
    "    probDF.to_csv(datasetsPath + 'compareCaptures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Main</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printConfusionMatrix()\n",
    "plotLossCurves()\n",
    "printClassReport()\n",
    "printMisc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genInfer(start=0, size=100, csv=True, float=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate C-Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portToC(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Inference Data and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateProbDF()      # params: localCapture (model to capture data from), features_test\n",
    "# exportProbDF()        # params: probDF (probDF export)\n",
    "# importInoCapture()    \n",
    "# generateComparison()  # params: probDF, inoCapture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
