{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Imports/Installs</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Installing required packages (if missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/har/heart_attack_risk_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspecting The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSummaryStatistics():   # Prints statistical for each column in the dataframe\n",
    "    dataCols = data.columns.to_list()\n",
    "    for col in dataCols:\n",
    "        print(f\"Column: {col} \\n{data[col].describe()} \\nData Type: {data[col].dtype}\\n\")\n",
    "\n",
    "print(f'{data.head()}\\n')       # Looking into basic structure\n",
    "printSummaryStatistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Data Preprocessing and Training</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leGender = LabelEncoder()       # Female, Male\n",
    "leHML = LabelEncoder()          # High, Moderate, Low\n",
    "leChestPain = LabelEncoder()    # Non-anginal, Asymptomatic, Typical, Atypical\n",
    "leThalassemia = LabelEncoder()  # Normal, Fixed Defect, Reversible Defect\n",
    "leECG = LabelEncoder()          # Normal, ST-T abnormality, Left ventricular hypertrophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Gender'] = leGender.fit_transform(data['Gender'])\n",
    "\n",
    "data['Physical_Activity_Level'] = leHML.fit_transform(data['Physical_Activity_Level'])\n",
    "data['Stress_Level'] = leHML.fit_transform(data['Stress_Level'])\n",
    "data['Heart_Attack_Risk'] = leHML.fit_transform(data['Heart_Attack_Risk'])\n",
    "\n",
    "data['Chest_Pain_Type'] = leChestPain.fit_transform(data['Chest_Pain_Type'])\n",
    "data['Thalassemia'] = leThalassemia.fit_transform(data['Thalassemia'])\n",
    "data['ECG_Results'] = leECG.fit_transform(data['ECG_Results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forming Dataset into Training, Test, Eval, Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.iloc[:,:19]       # Features\n",
    "y_data = data.iloc[:,19:]       # Labels\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x_data, \n",
    "    y_data, \n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "evaldata=[(xtrain,ytrain),(xtest,ytest)]          # Datensatz zur Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Improving<br>\n",
    "Um eine gute Anzahl an Estimators zu bestimmen, wird zuerst ein Modell mithilfe von Early Stopping, sowie einer großen Menge an Estimatoren trainiert. Hiermit wird die beste Anzahl an Iterationen ermittelt und mit dieser Anzahl ein weiteres Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor = XGBClassifier(              # \"Spendermodell\"\n",
    "    objective='multi:softmax',      # Multi-Klassifizierung\n",
    "    num_class=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=10000,             # \"Große Anzahl an Schaetzern, die nicht erreicht werden soll\"\n",
    "    early_stopping_rounds=50,       # Anzahl an Runden, bei denen sich das Modell nicht verbessern muss, bis abgebrochen wird\n",
    "    # max_depth=3                   # Erstmal weglassen\n",
    ")\n",
    "\n",
    "donor.fit( # Donor model\n",
    "    xtrain, \n",
    "    ytrain, \n",
    "    eval_set=evaldata, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "bIter = donor.best_iteration        # Beste Anzahl an Estimatoren\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softmax',  # Specify the multi-class classification task\n",
    "    num_class=3,                # Number of classes (Low, Moderate, High)\n",
    "    learning_rate=0.1,          # Learning rate for the model\n",
    "    n_estimators=bIter,         # Number of boosting rounds (iterations)\n",
    "    num_parallel_tree=1         # m2c workaround\n",
    "    # max_depth=3,              # Maximum depth of the trees\n",
    ")\n",
    "\n",
    "model.fit( # Final model\n",
    "    xtrain, \n",
    "    ytrain, \n",
    "    eval_set=evaldata, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(model.classes_)\n",
    "\n",
    "yhat = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Func Definitions</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(): # Confusion Matrix\n",
    "    metrics.ConfusionMatrixDisplay.from_estimator(model, xtest, ytest, cmap='Blues')\n",
    "    pyplot.show()\n",
    "\n",
    "def plotLossCurves():       # Loss Curves\n",
    "    # save evaluation results\n",
    "    results = model.evals_result()\n",
    "    # plot curves\n",
    "    lossValue = list(results['validation_1'])[0]\n",
    "    pyplot.plot(results['validation_0'][lossValue], label='train')\n",
    "    pyplot.plot(results['validation_1'][lossValue], label='train')\n",
    "    # show the legend\n",
    "    pyplot.xlabel('Iterations')\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "def printClassReport():     # Classification Report\n",
    "    # Report\n",
    "    print(metrics.classification_report(ytest, yhat, digits = 3))\n",
    "\n",
    "def printMisc():            # Best Iter, Test Accuracy, Base Score, Probas,\n",
    "    # Misc\n",
    "    print(f'# Trees / Best Iteration: \\t{bIter}')\n",
    "    print(f'Test Accuracy: \\t{accuracy_score(ytest, yhat)}')\n",
    "    print(f'Base_Score{model.base_score}')\n",
    "    print(f'\\nPredict_Proba Return: \\n{model.predict_proba(xtest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Porting this Bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portToC(model):\n",
    "    import m2cgen as m2c\n",
    "\n",
    "    with open('../exported_models/currentExport.c','w') as f:\n",
    "        code = m2c.export_to_c(model)\n",
    "        f.write(code)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating Code for Lazy People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInfer(start=0, size=500, csv=True, float=True):\n",
    "    start = start\n",
    "    size = size\n",
    "    length = 2\n",
    "    \n",
    "    # Declaring function\n",
    "    print(f'void infer(int time, int csv) {{')\n",
    "\n",
    "    # Printing Header\n",
    "    print(f'\\tif(csv==1){{')\n",
    "    print(f'\\t\\tSerial.println(\"aScore0,aScore1\");        // Printing header to name columns in csv')\n",
    "    print(f'\\t}} else {{')\n",
    "    print(f'\\t\\tSerial.println(\"Start: {start} | End: {start+size}\");    // Printing Range:')\n",
    "    print(f'\\t}}')\n",
    "\n",
    "    print(f'\\t// Declarations:')\n",
    "    print(f'\\tint length = {length};')\n",
    "    if float == True:\n",
    "        print(f'\\tfloat result[length];')\n",
    "    else:\n",
    "        print(f'\\tdouble result[length];')\n",
    "\n",
    "    print(f'\\t// Model Inference')\n",
    "    for x in range(start,(start+size)):  \n",
    "        \n",
    "        if float == True:\n",
    "            print(f'\\tfloat x_{x}[] = {{' , end=\"\")    \n",
    "        else:\n",
    "            print(f'\\tdouble x_{x}[] = {{' , end=\"\")\n",
    "\n",
    "        features = xtest.values[x]\n",
    "\n",
    "        for i in range(len(features)):\n",
    "            if i < (len(features)-1):\n",
    "                print(features[i], end=\", \")\n",
    "            else:\n",
    "                print(features[i], end=\"};\\n\")\n",
    "\n",
    "        if csv == False:\n",
    "            print(f'\\tint y_{x} = {yhat[x]};')\n",
    "\n",
    "        print(f'\\tscore(x_{x}, result);')\n",
    "        \n",
    "        if csv == True:\n",
    "            print(f'\\tprintScoreCSV(result, length);')\n",
    "        else:\n",
    "            print(f'\\tprintScoreCompare(result, length, y_{x});')\n",
    "        \n",
    "        print(f'\\tdelay(time);\\n')\n",
    "    print(f'}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Inference Data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProbDF(localCapture=model,features_test=xtest):\n",
    "    xtestlist = localCapture.predict_proba(features_test).tolist()\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "\n",
    "    for x in xtestlist:\n",
    "        list1.append(round(x[0],4))\n",
    "        list2.append(round(x[1],4))\n",
    "\n",
    "    probDF = pd.DataFrame({\n",
    "        'Label': localCapture.predict(features_test),\n",
    "        'Prob0': list1,\n",
    "        'Prob1': list2\n",
    "    })\n",
    "    return(probDF)\n",
    "\n",
    "def exportProbDF(probDF = generateProbDF()):\n",
    "    probDF.to_csv('../datasets/har/baseCapture.csv')\n",
    "\n",
    "def importInoCapture():\n",
    "    serial = pd.read_csv('../datasets/har/inoCapture.csv')\n",
    "    serial = serial.truncate(after=(len(serial)-2)) # get rid of ##### REPEATING... #####\n",
    "    return(serial)\n",
    "\n",
    "def generateComparison(probDF=generateProbDF(),inoCapture=importInoCapture()):\n",
    "    probDF = probDF.truncate(after=(len(inoCapture)-1))\n",
    "    probDF = probDF.join(inoCapture)\n",
    "    probDF.to_csv('../datasets/har/compareCaptures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Main</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printConfusionMatrix()\n",
    "plotLossCurves()\n",
    "printClassReport()\n",
    "printMisc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genInfer(start=0, size=10, csv=True, float=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate C-Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portToC(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Inference Data and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateProbDF()      # params: localCapture (model to capture data from), features_test\n",
    "# exportProbDF()        # params: probDF (probDF export)\n",
    "# importInoCapture()    \n",
    "# generateComparison()  # params: probDF, inoCapture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
