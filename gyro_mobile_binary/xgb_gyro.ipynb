{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Imports/Installs</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <ins>Installing required packages (if missing)</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Import required libs</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsPath = '../datasets/gyro/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Importing Dataset.csv</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro = pd.read_csv(datasetsPath + 'gyro_mobile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Inspecting The Dataset</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSummaryStatistics():   # Prints statistical for each column in the dataframe\n",
    "    gyroCols = gyro.columns.to_list()\n",
    "    for col in gyroCols:\n",
    "        print(f\"Column: {col} \\n{gyro[col].describe()} \\nData Type: {gyro[col].dtype}\\n\")\n",
    "\n",
    "print(f'{gyro.head()}\\n')       # Looking into basic structure\n",
    "printSummaryStatistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "- 31991 data points\n",
    "- Every feature is continuous\n",
    "- Activity is either 1 or 0 (binary classification)\n",
    "- Dataset contains a timestamp that might be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Data Preprocessing and Training</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Dropping timestamp and splitting data into Training, Testing and Eval</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro = gyro.drop(columns='timestamp')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    gyro.iloc[:,:6],\n",
    "    gyro.iloc[:,6:],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "evaldata=[(xtrain,ytrain),(xtest,ytest)]          # Datensatz zur Evaluierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Training and Improving</ins><br>\n",
    "Um eine gute Anzahl an Estimators zu bestimmen, wird zuerst ein Modell mithilfe von Early Stopping, sowie einer großen Menge an Estimatoren trainiert. Hiermit wird die beste Anzahl an Iterationen ermittelt und mit dieser Anzahl ein weiteres Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preModel = XGBClassifier(           # \"Spendermodell\"\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=10000,             # \"Große Anzahl an Schaetzern, die nicht erreicht werden soll\"\n",
    "    early_stopping_rounds=20,       # Anzahl an Runden, bei denen sich das Modell nicht verbessern muss, bis abgebrochen wird\n",
    "    max_depth=2,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "preModel.fit(\n",
    "    xtrain, \n",
    "    ytrain, \n",
    "    eval_set=evaldata, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "bIter = preModel.best_iteration     # Beste Anzahl an Estimatoren\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    # tree_method = 'exact',\n",
    "    n_estimators=bIter,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.1,\n",
    "    base_score=0.5\n",
    ")\n",
    "\n",
    "model.fit(xtrain, \n",
    "    ytrain, \n",
    "    eval_set=evaldata, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "yhat = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Func Definitions</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Performance Metrics and Evaluation</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(): # Confusion Matrix\n",
    "    metrics.ConfusionMatrixDisplay.from_estimator(model, xtest, ytest, cmap='Blues')\n",
    "    pyplot.show()\n",
    "\n",
    "def plotLossCurves():       # Loss Curves\n",
    "    # save evaluation results\n",
    "    results = model.evals_result()\n",
    "    # plot curves\n",
    "    lossValue = list(results['validation_1'])[0]\n",
    "    pyplot.plot(results['validation_0'][lossValue], label='train')\n",
    "    pyplot.plot(results['validation_1'][lossValue], label='train')\n",
    "    # show the legend\n",
    "    pyplot.xlabel('Iterations')\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "def printClassReport():     # Classification Report\n",
    "    # Report\n",
    "    print(metrics.classification_report(ytest, yhat, digits = 3))\n",
    "\n",
    "def printMisc():            # Best Iter, Test Accuracy, Base Score, Probas,\n",
    "    # Misc\n",
    "    print(f'# Trees / Best Iteration: \\t{bIter}')\n",
    "    print(f'Test Accuracy: \\t{accuracy_score(ytest, yhat)}')\n",
    "    print(f'Base_Score{model.base_score}')\n",
    "    print(f'\\nPredict_Proba Return: \\n{model.predict_proba(xtest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Porting this Bitch</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portToC(model):\n",
    "    import m2cgen as m2c\n",
    "\n",
    "    with open('../exported_models/currentExport.c','w') as f:\n",
    "        code = m2c.export_to_c(model)\n",
    "        f.write(code)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Code for Lazy People</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genInfer(start=0, size=500, csv=True, float=True):\n",
    "    start = start\n",
    "    size = size\n",
    "    length = 2\n",
    "    \n",
    "    # Declaring function\n",
    "    print(f'void infer(int time, int csv) {{')\n",
    "\n",
    "    # Printing Header\n",
    "    print(f'\\tif(csv==1){{')\n",
    "    print(f'\\t\\tSerial.println(\"aScore0,aScore1\");        // Printing header to name columns in csv')\n",
    "    print(f'\\t}} else {{')\n",
    "    print(f'\\t\\tSerial.println(\"Start: {start} | End: {start+size}\");    // Printing Range:')\n",
    "    print(f'\\t}}')\n",
    "\n",
    "    print(f'\\t// Declarations:')\n",
    "    print(f'\\tint length = {length};')\n",
    "    if float == True:\n",
    "        print(f'\\tfloat result[length];')\n",
    "    else:\n",
    "        print(f'\\tdouble result[length];')\n",
    "\n",
    "    print(f'\\t// Model Inference')\n",
    "    for x in range(start,(start+size)):  \n",
    "        if float == True:\n",
    "            print(f'\\tfloat x_{x}[] = {{' , end=\"\")    \n",
    "        else:\n",
    "            print(f'\\tdouble x_{x}[] = {{' , end=\"\")\n",
    "        features = xtest.values[x]\n",
    "        for i in range(len(features)):\n",
    "            if i < (len(features)-1):\n",
    "                print(features[i], end=\", \")\n",
    "            else:\n",
    "                print(features[i], end=\"};\\n\")\n",
    "        print(f'\\tint y_{x} = {yhat[x]};')\n",
    "        print(f'\\tscore(x_{x}, result);')\n",
    "        if csv == True:\n",
    "            print(f'\\tprintScoreCSV(result, length, y_{x});')\n",
    "        else:\n",
    "            print(f'\\tprintScoreCompare(result, length, y_{x});')\n",
    "        \n",
    "        print(f'\\tdelay(time);\\n')\n",
    "    print(f'}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Inference Data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProbDF(localCapture=model,features_test=xtest):\n",
    "    xtestlist = localCapture.predict_proba(features_test).tolist()\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "\n",
    "    for x in xtestlist:\n",
    "        list1.append(round(x[0],4))\n",
    "        list2.append(round(x[1],4))\n",
    "\n",
    "    probDF = pd.DataFrame({\n",
    "        'Label': localCapture.predict(features_test),\n",
    "        'Prob0': list1,\n",
    "        'Prob1': list2\n",
    "    })\n",
    "    return(probDF)\n",
    "\n",
    "def exportProbDF(probDF = generateProbDF()):\n",
    "    probDF.to_csv(datasetsPath + 'baseCapture.csv')\n",
    "\n",
    "def importInoCapture():\n",
    "    serial = pd.read_csv(datasetsPath + 'inoCapture.csv')\n",
    "    serial = serial.truncate(after=(len(serial)-2)) # get rid of ##### REPEATING... #####\n",
    "    return(serial)\n",
    "\n",
    "def generateComparison(probDF=generateProbDF(),inoCapture=importInoCapture()):\n",
    "    probDF = probDF.truncate(after=(len(inoCapture)-1))\n",
    "    probDF = probDF.join(inoCapture)\n",
    "    probDF.to_csv(datasetsPath + 'compareCaptures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Main</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Evaluation Metrics</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printConfusionMatrix()\n",
    "# plotLossCurves()\n",
    "# printClassReport()\n",
    "# printMisc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Code Generation</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genInfer(start=0, size=500, csv=True, float=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generate C-Port</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portToC(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generate Inference Data and Comparison</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateProbDF()      # params: localCapture (model to capture data from), features_test\n",
    "# exportProbDF()        # params: probDF (probDF export)\n",
    "# importInoCapture()    \n",
    "generateComparison()  # params: probDF, inoCapture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
