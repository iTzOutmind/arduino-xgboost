{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Imports/Installs</ins>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### <ins>Installing required packages (if missing)</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Import required libs</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Importing Dataset.csv</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro = pd.read_csv('../datasets/gyro/gyro_mobile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Inspecting The Dataset</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSummaryStatistics():   # Prints statistical for each column in the dataframe\n",
    "    gyroCols = gyro.columns.to_list()\n",
    "    for col in gyroCols:\n",
    "        print(f\"Column: {col} \\n{gyro[col].describe()} \\nData Type: {gyro[col].dtype}\\n\")\n",
    "\n",
    "print(f'{gyro.head()}\\n')       # Looking into basic structure\n",
    "printSummaryStatistics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "- 31991 data points\n",
    "- Every feature is continuous\n",
    "- Activity is either 1 or 0 (binary classification)\n",
    "- Dataset contains a timestamp that might be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Data Preprocessing and Training</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Dropping timestamp and splitting data into training and testing</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyro = gyro.drop(columns='timestamp')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    gyro.iloc[:,:6],\n",
    "    gyro.iloc[:,6:],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Training and Improving</ins><br>\n",
    "Um eine gute Anzahl an Estimators zu bestimmen, wird zuerst ein Modell mithilfe von Early Stopping, sowie einer großen Menge an Estimatoren trainiert. Hiermit wird die beste Anzahl an Iterationen ermittelt und mit dieser Anzahl ein weiteres Modell trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preModel = XGBClassifier(           # \"Spendermodell\"\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=10000,             # \"Große Anzahl an Schaetzern, die nicht erreicht werden soll\"\n",
    "    early_stopping_rounds=20,       # Anzahl an Runden, bei denen sich das Modell nicht verbessern muss, bis abgebrochen wird\n",
    "    max_depth=2,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "evaldata=[(xtrain,ytrain),(xtest,ytest)]          # Datensatz zur Evaluierung\n",
    "\n",
    "preModel.fit(xtrain, ytrain, eval_set=evaldata, verbose=False)\n",
    "\n",
    "bIter = preModel.best_iteration     # Beste Anzahl an Estimatoren\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    # tree_method = 'exact',\n",
    "    n_estimators=bIter,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.1,\n",
    "    base_score=0.5\n",
    ")\n",
    "\n",
    "model.fit(xtrain, ytrain, eval_set=evaldata, verbose=False)\n",
    "\n",
    "yhat = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Func Definitions</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Performance Metrics and Evaluation</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(): # Confusion Matrix\n",
    "    metrics.ConfusionMatrixDisplay.from_estimator(model, xtest, ytest, cmap='Blues')\n",
    "    pyplot.show()\n",
    "\n",
    "def plotLossCurves():       # Loss Curves\n",
    "    # save evaluation results\n",
    "    results = model.evals_result()\n",
    "    # plot curves\n",
    "    pyplot.plot(results['validation_0']['logloss'], label='train')\n",
    "    pyplot.plot(results['validation_1']['logloss'], label='train')\n",
    "    # show the legend\n",
    "    pyplot.xlabel('Iterations')\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "def printClassReport(): # Classification Report\n",
    "    # Report\n",
    "    print(metrics.classification_report(ytest, yhat, digits = 3))\n",
    "\n",
    "def printMisc():\n",
    "    # Misc\n",
    "    print(f'# Trees: \\t{bIter}')\n",
    "    print(f'Test Accuracy: \\t{accuracy_score(ytest, yhat)}')\n",
    "    print(f'Base_Score{model.base_score}')\n",
    "    print(f'Best Iteration: {bIter}')\n",
    "    print(f'\\nPredict_Proba Return: \\n{model.predict_proba(xtest)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Porting this Bitch</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portToC(model):\n",
    "    import m2cgen as m2c\n",
    "\n",
    "    with open('../exported_models/currentExport.c','w') as f:\n",
    "        code = m2c.export_to_c(model)\n",
    "        f.write(code)\n",
    "\n",
    "portToC(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Code for Lazy People</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFloat(start=0, size=500, time=100):\n",
    "    start = start\n",
    "    size = size\n",
    "    time = time\n",
    "    length = 2\n",
    "    \n",
    "\n",
    "    print(f'void infer() {{')\n",
    "    print(f'\\t// Printing Range:')\n",
    "    print(f'\\tSerial.println(\\\"Start: {start} | End: {start+size}\\\");\\n')\n",
    "    print(f'\\tSerial.println(\"aScore0,aScore1\");')\n",
    "\n",
    "    print(f'\\t// Declarations:')\n",
    "    print(f'\\tint length = {length};')\n",
    "    print(f'\\tfloat result[length];')\n",
    "    print(f'\\tint time = {time};\\n')\n",
    "\n",
    "    print(f'\\t// Model Inference')\n",
    "    for x in range(start,(start+size)):  \n",
    "        print(f'\\tfloat x_{x}[] = {{' , end=\"\")    \n",
    "        features = xtest.values[x]\n",
    "        for i in range(len(features)):\n",
    "            if i < (len(features)-1):\n",
    "                print(features[i], end=\", \")\n",
    "            else:\n",
    "                print(features[i], end=\"};\\n\")\n",
    "        print(f'\\tint y_{x} = {yhat[x]};')\n",
    "        print(f'\\tscore(x_{x}, result);')\n",
    "        # print(f'\\tprintScoreCompare(result, length, y_{x});')\n",
    "        print(f'\\tprintScoreCompareCSV(result, length, y_{x});')\n",
    "        print(f'\\tdelay(time);\\n')\n",
    "    print(f'}}')\n",
    "\n",
    "def genDouble(start=0, size=500, time=100):\n",
    "    start = start\n",
    "    size = size\n",
    "    time = time\n",
    "    length = 2\n",
    "\n",
    "    print(f'void infer() {{')\n",
    "    print(f'\\t// Printing Range:')\n",
    "    print(f'\\tSerial.println(\\\"Start: {start} | End: {start+size}\\\");\\n')\n",
    "    print(f'\\tSerial.println(\"aScore0,aScore1\");')\n",
    "    print(f'\\t// Declarations:')\n",
    "    print(f'\\tint length = {length};')\n",
    "    print(f'\\tdouble result[length];')\n",
    "    print(f'\\tint time = {time};\\n')\n",
    "\n",
    "    print(f'\\t// Model Inference')\n",
    "    for x in range(start,(start+size)):\n",
    "        print(f'\\tdouble x_{x}[] = {{' , end=\"\")        \n",
    "        features = xtest.values[x]\n",
    "        for i in range(len(features)):\n",
    "            if i < (len(features)-1):\n",
    "                print(features[i], end=\", \")\n",
    "            else:\n",
    "                print(features[i], end=\"};\\n\")\n",
    "        print(f'\\tint y_{x} = {yhat[x]};')\n",
    "        print(f'\\tscore(x_{x}, result);')\n",
    "        # print(f'\\tprintScoreCompare(result, length, y_{x});')\n",
    "        print(f'\\tprintScoreCompareCSV(result, length, y_{x});')\n",
    "        print(f'\\tdelay(time);\\n')\n",
    "    print(f'}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generating Inference Data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateProbDF(localCapture=model,features_test=xtest):\n",
    "    xtestlist = localCapture.predict_proba(features_test).tolist()\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "\n",
    "    for x in xtestlist:\n",
    "        list1.append(round(x[0],4))\n",
    "        list2.append(round(x[1],4))\n",
    "\n",
    "    probDF = pd.DataFrame({\n",
    "        'Label': localCapture.predict(features_test),\n",
    "        'Prob0': list1,\n",
    "        'Prob1': list2\n",
    "    })\n",
    "    return(probDF)\n",
    "\n",
    "def exportProbDF(probDF = generateProbDF()):\n",
    "    probDF.to_csv('../datasets/gyro/baseCapture.csv')\n",
    "\n",
    "def importInoCapture():\n",
    "    serial = pd.read_csv('../datasets/gyro/inoCapture.csv')\n",
    "    serial = serial.truncate(after=(len(serial)-2)) # get rid of ##### REPEATING... #####\n",
    "    return(serial)\n",
    "\n",
    "def generateComparison(probDF=generateProbDF(),inoCapture=importInoCapture()):\n",
    "    probDF = probDF.truncate(after=(len(inoCapture)-1))\n",
    "    probDF = probDF.join(inoCapture)\n",
    "    probDF.to_csv('../datasets/gyro/compared-gyro-float.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <ins>Main</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Evaluation Metrics</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printConfusionMatrix()\n",
    "# plotLossCurves()\n",
    "# printClassReport()\n",
    "# printMisc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Code Generation</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genFloat()    # param: size\n",
    "# genDouble()   # param: size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generate C-Port</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portToC(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <ins>Generate Inference Data and Comparison</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generateProbDF()\n",
    "# importInoCapture()\n",
    "# exportProbDF()\n",
    "# generateComparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
